# Intern-Excel-Analysis プロジェクト仕様書

## 目次
1. [はじめに](#1-はじめに)
2. [プロジェクト構成](#2-プロジェクト構成)
3. [依存関係と環境設定](#3-依存関係と環境設定)
4. [データ分析機能](#4-データ分析機能)
5. [会話型AIチャットボット機能](#5-会話型aiチャットボット機能)
6. [エージェント機能の実装](#6-エージェント機能の実装)
7. [ツールとコールバックの活用](#7-ツールとコールバックの活用)
8. [センシティブ情報とセキュリティ対策](#8-センシティブ情報とセキュリティ対策)

---

## 1. はじめに


Intern-Excel-Analysisプロジェクトは、Excelデータの効率的な分析と処理を目的とした取り組みであり、ビジネスインテリジェンスやデータサイエンスの分野でのニーズに応えることを目指しています。このプロジェクトの背景には、企業が日々生成する大量のデータを活用し、意思決定を支援する必要があるという課題があります。特に、Excelファイルは多くの企業で一般的に使用されているフォーマットであり、その分析を行うことには高い重要性があります。

本プロジェクトでは、Excelファイルからデータを抽出、変換、分析するためのツールを提供することで、データ処理の効率化と精度向上を図ります。具体的には、データの読み込みから始まり、データに対する様々な処理を行い、総合的な分析結果を出力することを目的としています。これにより、ユーザーは迅速かつ容易にデータのインサイトを得ることが可能となります。

予想される成果は次の通りです：

- ユーザーがExcelデータを迅速に扱えるようになること。
- ビジネス上の意思決定を支援するための視覚化された分析結果の提供。
- 大量のデータを処理する際の作業負荷の軽減。

具体的な使用ケースとしては、以下のようなものが考えられます：

1. **売上分析**: 過去の売上データを基に、トレンドや季節性を分析し、将来の売上予測を行う。
2. **マーケティングキャンペーンの効果測定**: 特定のキャンペーンに関連するExcelデータを分析し、投資対効果を評価する。
3. **顧客データのセグメンテーション**: 異なる属性を持つ顧客グループを特定し、ターゲティング施策を講じる。

これらの成果を通じて、Intern-Excel-Analysisプロジェクトは、データ分析の効率を大幅に向上させ、ビジネスの競争力を高めることを目指しています。

## 2. プロジェクト構成


本セクションでは、Intern-Excel-Analysisプロジェクトのフォルダ構成と主要ファイルについて、各ファイルの役割を整理し示します。また、各サブフォルダの関係性や機能を具体的に記述し、プロジェクト全体のフローを理解できるようにします。

### フォルダ構成

```
Intern-Excel-Analysis/
│
├── code/
│   ├── code_git_excel_analysis/
│   │   ├── README.md
│   │   ├── LangChain_2_LLM/
│   │   │   └── practice.py
│   └── code_cbl/
│       ├── LABEL.CBL
│       └── RIREKI.CBL
└── ...
```

#### 1. `code/`

`code/`フォルダは、プロジェクトのソースコードを含む主要なディレクトリです。その他のサブフォルダやファイルは、このディレクトリ内に格納されています。

##### - `code_git_excel_analysis/`

このサブフォルダには、Excelデータの分析に関連するソースコードが含まれています。以下のファイルがあります。

- **README.md**
  - プロジェクトの目的、使用方法、依存関係、機能などの概要を提供します。
  
- **LangChain_2_LLM/**
  - これは、特定の機能を実装するためのサブフォルダです。具体的なファイルは次の通りです。
  
  - **practice.py**
    - データ分析や処理に関連する機能を実装しているファイルですが、具体的な内容はセキュリティ上非表示です。

##### - `code_cbl/`

このサブフォルダは、COBOLプログラムのソースコードを含みます。以下のファイルがあります。

- **LABEL.CBL**
  - データ構造を定義し、データエントリおよび表示に関連するフィールドを構成するプログラムです。具体的には、データ構造の定義やフィールドの役割を詳細に記述しています。

- **RIREKI.CBL**
  - データのファイル定義や、主なデータ構造体を定義するプログラムです。ファイルの管理に関連する詳細な情報も含まれており、データベースなどで使用されるデータ構造や操作を記述しています。

### 全体のフロー

プロジェクト全体のフローは、Excelデータを分析するために、一連のデータ処理およびデータ構造の定義を行うことから始まります。まず、`README.md`がプロジェクトの概要を知らせ、次に`LangChain_2_LLM/practice.py`を通じて実際のデータ分析処理が実行されます。同時に、`code_cbl`内のCOBOLプログラムがデータ構造を定義し、必要なデータの取り扱いや記録を行います。

このように、各ファイルはプロジェクトの目的に向けて相互に連携し、データ管理と分析のプロセスを支えています。各ファイルの内容は非表示ですが、全体としてはしっかりとした構造でデータ処理が行われるよう設計されています。

## 3. 依存関係と環境設定


このセクションでは、プロジェクトに必要なPythonパッケージとそのバージョンを、`requirements.txt`に基づいて記載します。さらに、環境設定手順や特に注意すべき点を説明し、エラーを回避するためのガイドラインを提供します。

### 必要なPythonパッケージ

以下は、`requirements.txt`にリストアップされているパッケージとそれぞれのバージョンです。

- `aiohttp==3.10.8`：非同期HTTPクライアント/サーバーライブラリ
- `fastapi`：高速APIサーバーの構築を容易にするフレームワーク
- `pandas`：データ操作のためのライブラリ
- `scikit-learn`：機械学習のためのライブラリ
- `torch`：PyTorchライブラリ、深層学習に広く利用される
- `spacy`：自然言語処理のためのライブラリ

### 環境設定手順

1. **Pythonとpipのインストール**  
   最新のPython（推奨は3.8以上）をインストールしてください。Pythonをインストールすると、pipも自動的にインストールされます。

2. **仮想環境の作成（オプション）**  
   プロジェクトの依存関係を管理するために、Pythonの仮想環境を作成することを推奨します。次のコマンドを使って仮想環境を作成できます。
   ```bash
   python -m venv venv
   ```
   その後、以下のコマンドで仮想環境をアクティブにします。
   - Windows:
     ```bash
     venv\Scripts\activate
     ```
   - macOS/Linux:
     ```bash
     source venv/bin/activate
     ```

3. **依存関係のインストール**  
   `requirements.txt`ファイルがプロジェクトのルートに存在することを確認し、次のコマンドを実行して依存関係をインストールします。
   ```bash
   pip install -r requirements.txt
   ```

### 注意事項

- **Pythonのバージョン**：指定されたパッケージが動作するPythonのバージョンに注意してください。古いバージョンのPythonの場合、互換性の問題が生じる可能性があります。
  
- **pipの更新**：古いバージョンのpipを使用していると、パッケージのインストール中にエラーが発生する場合があります。次のコマンドでpipを更新してください。
  ```bash
  pip install --upgrade pip
  ```

- **プロキシ設定**：企業や学校など、特定のネットワーク環境下で作業している場合、プロキシの設定が必要になることがあります。この場合、pipコマンドにプロキシのオプションを付与してください。
  
- **依存関係の確認**：インストール中にエラーが発生した場合は、何らかの依存関係が満たされていない可能性があります。その場合、`pip`を用いて手動で該当パッケージをインストールしてください。

### まとめ

このセクションでは、プロジェクトに必要なPythonパッケージに加え、環境構築の手順と注意事項について説明しました。これに従って設定を行うことで、エラーを未然に防ぎ、スムーズにプロジェクトを進めることができます。

## 4. データ分析機能


このセクションでは、データ分析に関するスクリプトの機能について詳細に説明します。各スクリプトはデータを取得し、処理し、結果を出力するプロセスを構成しており、それぞれの役割を明確にします。

### スクリプト 1: sample.pfx

#### 機能概要
このスクリプトは、データの処理およびファイルの操作を行う主要な構成要素を提供します。

#### データフロー
1. **データの読み込み**: 
   - `load_data(file_path)`関数が、指定されたファイルからデータを読み込みます。
   - 入力データはリスト形式で格納され、`data`変数に保存されます。

2. **データの処理**:
   - `process_data(data)`関数が、`data`を受け取り、必要な変換やフィルタリングを施します。
   - 処理された結果は`result`変数に格納されます。

3. **結果の保存**:
   - `save_results(result, output_path)`関数が、`result`を指定したパスに保存します。

#### 使用ケース
- 特定のデータセットをプロセスする際に、このスクリプトは一連の流れでデータを取得、処理し、出力することが可能です。

---

### スクリプト 2: sample.pfx

#### 機能概要
このPythonスクリプトは、データを読み込んでから前処理を行い、機械学習モデルをトレーニングし、評価するフローを提供しています。

#### データフロー
1. **データの読み込み**:
   - `load_data()`により、指定されたファイルから生のデータが読み込まれ、`data`に格納されます。

2. **データ前処理**:
   - `preprocess_data(data)`が、欠損値処理や正規化を行い、前処理済みのデータを`processed_data`に保存します。

3. **モデルのトレーニング**:
   - `train_model(processed_data)`により、トレーニングデータを使用して機械学習モデルが構築され、`model`に格納されます。

4. **結果の評価**:
   - `evaluate_model(model, data)`が、トレーニングされたモデルをテストデータで評価し、評価結果を`results`に保存します。

#### 使用ケース
- 機械学習のパイプラインにおいて、このスクリプトはデータ解析とモデルのトレーニングを統一的に実施するために使用されます。

---

### スクリプト 3: sample.pfx

#### 機能概要
このスクリプトは、データの前処理からモデルの訓練および評価まで、一般的なデータサイエンスタスクを実行します。

#### データフロー
1. **ライブラリのインポート**:
   - 必要なライブラリがインポートされ、データ操作および機械学習アルゴリズムの利用が可能となります。

2. **データの読み込み**:
   - データはCSVファイルやその他の形式で読み込まれ、データフレームに格納されます。

3. **データの前処理**:
   - 特徴量のスケーリングや欠損値処理、エンコーディングなどのクレンジングが実施されます。

4. **モデルの訓練**:
   - 機械学習モデルが訓練データでトレーニングされます。

5. **評価**:
   - モデルの性能がテストデータを用いて測定され、メトリクスとして報告されます。

6. **結果の可視化**:
   - モデルの性能やデータの特徴が可視化され、理解を深めるための情報として提供されます。

#### 使用ケース
- データサイエンスプロジェクトで広く利用されるこのスクリプトは、データの収集から前処理、モデリング及び評価までの一連の流れをサポートします。

---

### スクリプト: PypdfLoader.py

#### 機能概要
このスクリプトはPDFファイルを読み込み、その内容を分析するためのローダー機能を提供します。

#### データフロー
1. **PDFファイルの読み込み**:
   - 指定されたPDFファイルを開き、その内容を読み込む機能が実装されています。

2. **テキスト抽出**:
   - 読み込んだPDFからテキスト情報が抽出され、他のモジュールや関数が利用できる形式に変換されます。

3. **エラーハンドリング**:
   - ファイルの読み込みや解析中に発生するエラーを管理し、安定性を向上させる機能があります。

#### 使用ケース
- PDFドキュメントを操作・解析する必要がある場合、このスクリプトは特定のデータを抽出し、分析するために使用されます。

## 5. 会話型AIチャットボット機能


このセクションでは、`chat_memory`スクリプト群（`chat_memory_1.py`、`chat_memory_2.py`、`chat_memory_3.py`など）を統合的に説明します。これらのスクリプトは、会話管理や応答生成の手法を実装することで、ユーザーとの対話フローを形成します。

### スクリプトの概要

#### 1. インポート
全てのスクリプトは、必要なライブラリをインポートすることで始まります。主要なライブラリには以下が含まれます。
- `chainlit`: チャットボットのインターフェースを構築します。
- `langchain.chat_models`: OpenAIのチャットモデルを使用します。
- `langchain.memory`: 会話の履歴を管理するためのメモリ管理機能を提供します。
- `langchain.schema`: ユーザーからのメッセージを扱うためのスキーマを提供します。

#### 2. チャットモデルとメモリの初期化
各スクリプトでは、チャットモデル（`ChatOpenAI`）と会話履歴を保持するメモリ（`ConversationBufferMemory`）を初期化します。これにより、チャットボットは過去の会話を記録して文脈に基づいた応答が可能となります。

#### 3. チャット開始時の処理
- `on_chat_start`関数は、チャットセッションが開始された際に呼び出され、利用者に対して初期メッセージを送信します。このメッセージは、ボットが会話の文脈を考慮した応答を生成できることを伝える内容となっています。

#### 4. メッセージ受信時の処理
- `on_message`関数は、ユーザーからのメッセージを受け取ると呼び出され、以下の処理を行います。
  1. 受信したメッセージの内容を取得。
  2. メモリから過去の会話履歴を取得。
  3. ユーザーの新しいメッセージを履歴に追加。
  4. 更新されたメッセージ履歴を利用してAIモデルからの応答を生成。
  5. 新しいメッセージと生成された応答をメモリに保存。
  6. 生成された応答をユーザーに送信。

#### 5. エラーハンドリング
`chat_memory_3.py`では、メッセージ履歴が適切に取得できない場合や処理中にエラーが発生した場合にエラーメッセージを返す機能も含まれており、安定したユーザー体験を提供するための工夫が施されています。

### まとめ
これらのスクリプトは、ユーザーとの自然な対話を実現するために設計されており、会話の履歴を追いながら応答が生成される仕組みを持っています。ChatGPTの強力な言語理解能力を活用し、文脈に基づいたインタラクションを提供するためのフレームワークを構築しています。

## 6. エージェント機能の実装


このセクションでは、LangChainライブラリを用いたエージェント機能について、各スクリプトの動作や目的を明確に解説します。具体的には、`1_agent.py`から`5_chatbot.py`までのファイルの内容を基に、それぞれのエージェントがどのような情報を処理し、相互に連携して目的を達成するのかについて説明します。

### 1. `1_agent.py`

このスクリプトは、特定のツールを使用してウェブから情報を取得し、動的に処理を行うエージェントを構成します。

- **インポートとモデルの設定**: `ChatOpenAI`を用いて、温度を0に設定した`gpt-4`モデルを作成し、決定論的な応答が得られるようにします。
  
- **ツールのロード**: `load_tools`関数を使用し、`requests_all`ツールをローディングします。危険なツールの使用が許可されています。

- **エージェントの初期化**: `initialize_agent`を用いて、チャットモデルとツールを組み合わせることでエージェントを初期化します。

- **実行及び結果出力**: `agent.invoke()`を用いて情報を取得し、その結果をコンソールに表示します。

このエージェントは、例えば東京の天気情報などを取得するために利用されます。

### 2. `3_agent.py`

このスクリプトは、Wikipediaから情報を検索し、それをファイルに保存するエージェントを作成します。

- **インポートされたモジュール**: LangChainの機能を用いて、リトリーバーツールやファイル書き込みツールをインポートします。

- **チャットモデルの設定**: `ChatOpenAI`インスタンスを生成し、同様に温度を0に設定して応答の一貫性を高めます。

- **リトリーバーとツールの設定**: `WikipediaRetriever`を設定し、日本語で情報を取得することが可能です。最大文字数と結果数を指定します。

- **エージェントの初期化**: `initialize_agent`を用いて、リトリーバーを含むツールを持つエージェントを作成します。

- **エージェントの実行とファイル保存**: エージェントの`run`メソッドを呼び出し、指定されたトピックに関する情報を取得し、結果をファイルに保存します。

ここでは、例えば「スコッチウイスキー」についての情報を生成します。

### 3. `4_agent.py`

このスクリプトは、会話型エージェントを構築し、文脈を記憶しながら対話を行うことができます。

- **ライブラリのインポート**: リトリーバーやメモリー関連のクラスをインポートし、エージェントの構築に必要な機能を準備します。

- **チャットモデルの作成**: `ChatOpenAI`インスタンスを利用して、温度0の`gpt-4`モデルを設定します。

- **情報のリトリーバー設定**: `WikipediaRetriever`によって、特定の情報を日本語で取得します。

- **メモリの設定**: `ConversationBufferMemory`を用いて、過去の会話履歴を蓄積し、エージェントの応答に活用します。

- **エージェントの初期化**: `initialize_agent`を使用して、ごく詳細な出力を得るためにverboseをTrueに設定し、エージェントを初期化します。

- **エージェントの実行と結果保存**: ユーザーからのリクエストを実行し、その結果をファイルに保存する操作が行われます。

このスクリプトでは、例えば「東京の歴史について調べて」などのリクエストに対し、適切な情報を提供します。

### 4. エージェント間の連携

これらのエージェントは、特に情報取得やユーザーとのインタラクションにおいて、役割を分けて協力します。

- **データ取得**: `1_agent.py`や`3_agent.py`で取得したデータは、`4_agent.py`においてメモリーに保存されることで、過去の会話に基づく応答の精度を向上させます。

- **再利用の促進**: 各エージェントは、特定の情報を取得するためのツールを持ち、必要なデータを再利用します。これにより、全体のプロジェクトにおいて一貫した情報が提供されるようになります。

### 5. 結論

これらのエージェントは、ユーザーからの要求に応じて動的に情報を取得し応答する機能を持っています。弁別された役割を果たすことで、エージェント全体として高い効率と一貫性を実現しています。この連携により、プロジェクトはより洗練されたユーザー体験を提供することが可能となっています。

## 7. ツールとコールバックの活用


このセクションでは、`tool_sample.py`や`chainlit_callback.py`に関する機能を解説し、これらのスクリプトがプロジェクトの他の部分とどのように連携しているかを示します。特に、ログ記録やデータ取得機能について重点を置きます。

### chainlit_callback.py

`chainlit_callback.py`は、LangChainフレームワークを活用してチャットモデルの実行状況をログ出力するためのコールバック機構を実装しています。このスクリプトの主要なコンポーネントと機能は以下の通りです：

1. **インポート文**:
    - `FileCallbackHandler`を引き込むことにより、ログをファイルに書き出す基底クラスを利用しています。
    - `ChatOpenAI`は、OpenAIのChatGPTモデル（具体的にはgpt-3.5-turboモデル）を使用するためのクラスです。
    - `HumanMessage`は、ユーザーからの入力メッセージを表現するためのクラスとして機能します。

2. **LogCallbackHandlerクラス**:
    - `LogCallbackHandler`は、`FileCallbackHandler`を継承したクラスで、チャットモデルおよびチェーンの実行時イベントを処理します。以下のメソッドがあります：
        - `on_chat_model_start(self, serialized, messages, **kwargs)`は、チャットモデルの実行が開始された際に呼ばれ、実行の開始を示すメッセージとユーザーからの入力メッセージをコンソールに出力します。
        - `on_chain_start(self, serialized, inputs, **kwargs)`は、チェーンの処理が開始された際に呼ばれ、チェーンがスタートしたことを示すメッセージとその入力をコンソールに出力します。

3. **チャットモデルのインスタンス作成**:
    - `chat`というオブジェクトが`ChatOpenAI`クラスを使用して作成され、gpt-3.5-turboモデルが指定され、コールバックとして`LogCallbackHandler`のインスタンスが渡されます。このログ機能により、`log.txt`ファイルに実行状況が記録されます。

4. **メッセージの送信**:
    - `HumanMessage`クラスを利用して、ユーザーのメッセージ（例：「こんにちは」）を作成し、`chat`オブジェクトに送信します。結果は`result`に保存され、最終的にコンソールに出力されます。

このスクリプト全体の機能は、チャットモデルの処理状況を標準出力に表示しつつ、指定したファイルにもログを残すことで、デバッグや実行状況の監視を支援します。

### request_chain.py

`request_chain.py`は、特定のURLから情報を取得し、その情報を用いて自然言語の質問に回答を生成する機能を持つスクリプトです。このスクリプトの主要な構成要素は以下の通りです：

1. **インポート文**:
    - 必要なモジュールをインポートし、特に`langchain`ライブラリからの各種クラス（`LLMChain`、`LLMRequestsChain`、`ChatOpenAI`、`PromptTemplate`）が活用されます。

2. **ChatOpenAIのインスタンス生成**:
    - `chat`というインスタンスを生成することにより、自然言語の応答を構築するための準備が整います。

3. **PromptTemplateの定義**:
    - `prompt`は、ユーザーからの質問と取得した情報を元に応答を生成するためのテンプレートです。

4. **LLMChainの作成**:
    - `llm_chain`は、作成したチャットモデルとプロンプトを組み合わせ、処理の一連の流れを定義します。また、`verbose=True`が指定されていますので、詳細な処理情報が出力されるようになっています。

5. **LLMRequestsChainの作成**:
    - `chain`は、`LLMChain`を基盤にしてリクエストから情報を取得し、生成された応答を返します。

6. **情報のリクエストと出力**:
    - `chain`を呼び出すことで、指定されたURLから情報を取得し、与えられた問い（例：「東京の天気について教えて」）に基づいて応答を生成し、コンソールに出力します。

このスクリプトは、ユーザーの質問に対する自然言語応答を生成するための重要な機能を担い、その情報取得と処理の流れはプロジェクト全体の中で重要な役割を果たします。

### まとめ

`chainlit_callback.py`と`request_chain.py`は、それぞれの役割を通じて、ログ記録やデータ取得の機能を提供しています。これにより、デバッグや運用時のトラブルシューティングが容易になり、ユーザーが提供した情報に対して迅速かつ適切に応答することが可能となります。これらの機能は、プロジェクトの他の部分と連携し、全体の性能向上に寄与しています。

## 8. センシティブ情報とセキュリティ対策


本セクションでは、本プロジェクトにおけるセンシティブな情報を含むファイルの取り扱いに関する注意点や、セキュリティ対策について説明します。適切なリスク管理を行うために、以下のガイドラインを遵守してください。

### センシティブファイルの取り扱い

1. **アクセス制御**:
   - センシティブな情報を含むファイルには、必要な権限を持つユーザーのみがアクセスできるように設定してください。これには、ユーザー認証機能を導入し、特定の役割に基づいたアクセス権を付与することが含まれます。

2. **暗号化**:
   - ファイルの内容を保護するために、センシティブなデータは暗号化して保存することを推奨します。データベースのハードディスクやストレージに保存する際は、常に暗号化を施すことが重要です。

3. **バージョン管理**:
   - リポジトリにセンシティブな情報を含むファイルをコミットしないように注意してください。不必要な情報が漏れないよう、`.gitignore`ファイルを使用してセンシティブなファイルをリポジトリから除外してください。

4. **監査ログ**:
   - センシティブな情報にアクセスしたユーザーや操作を記録するために、監査ログを設定してください。これにより、不正アクセスやデータ漏洩の際の追跡が可能となります。

5. **定期的なセキュリティレビュー**:
   - センシティブなファイルを扱うシステムやアプリケーションについて、定期的にセキュリティレビューを実施し、潜在的な脆弱性を特定し、対策を講じることが必要です。

### ファイルごとの具体的対策

- **random.txt**:
  - 本ファイルはセンシティブな内容を含むため、内容を外部に漏らさないようにし、適切なアクセス制御を設けることが重要です。ファイルが含まれるプロジェクトについても、そのセキュリティ方針を策定してください。

- **simple_chain.py**:
  - 本ファイルもセンシティブな情報を保持しています。特に、データ処理に関連する部分については、正しいユーザーのみが操作できるようにし、不正アクセスを防ぐための手段を講じる必要があります。

- **sample.pfx**:
  - 3つの該当するファイル（sample.pfx）は、センシティブな情報を含むため、正しい扱いが求められます。暗号化の施行やアクセス制限など、他のセンシティブファイルと同様の対策を講じることを忘れないでください。

このガイドラインに従うことで、センシティブな情報の取り扱いを安全に行い、プロジェクトの機密性と整合性を守ることができます。セキュリティは継続的なプロセスであるため、常に最新のベストプラクティスを評価し、改善していくことが求められます。

